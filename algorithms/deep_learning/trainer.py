import os, math, argparse, random, sys
from pathlib import Path
from multiprocessing import freeze_support
from typing import Optional, Callable

import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torch.amp.grad_scaler import GradScaler
from torch.amp.autocast_mode import autocast
import matplotlib.pyplot as plt
import numpy as np

from .dataset import VectorWatermarkSet
from .encoder import AdvVectorEncoder
from .decoder import AdvVectorDecoder
from .noise_layers import Compose, GaussianNoise, Quantize, DimMask
from configs.config import Config


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŠ¨æ€ Î»_MSE â”€â”€â”€â”€â”€â”€â”€â”€â”€
def lambda_mse(epoch: int, total: int) -> float:
    return max(0.5, 2 * (1 - epoch / total))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¯„ä¼°å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€
@torch.no_grad()
def evaluate(model_tuple, loader, noise, lam, device):
    enc, dec = model_tuple
    enc.eval()
    dec.eval()
    tot_loss = tot_bce = tot_ber = 0.0
    n = 0
    for cover, msg in loader:
        cover = cover.to(device).float()
        msg = msg.to(device).float()
        
        # âš ï¸ å…³é”®ä¿®å¤ï¼šæ·»åŠ å½’ä¸€åŒ–å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        norm = torch.norm(cover, p=2, dim=-1, keepdim=True)
        cover = cover / (norm + 1e-8)

        stego = enc(cover, msg)
        stego_noisy = noise(stego)  # åœ¨è¯„ä¼°ä¸­åº”ç”¨å™ªå£°
        logits = dec(stego_noisy)

        probs = torch.sigmoid(logits.float())  # åª sigmoid ä¸€æ¬¡
        bce = F.binary_cross_entropy_with_logits(
            logits.float(),  # AMP è‡ªåŠ¨å¤„ç†ç²¾åº¦
            msg  # ç›®æ ‡ bits (0/1)
        )
        mse = F.mse_loss(torch.tanh(logits) * 0, torch.tanh(logits) * 0)  # dummy
        loss = bce + lam * mse  # mse æ­¤å¤„æ— ç”¨ï¼Œåªä¿æŒå…¬å¼ä¸€è‡´
        ber = (probs > .5).float().ne(msg).float().mean()

        tot_loss += loss.item()
        tot_bce += bce.item()
        tot_ber += ber.item()
        n += 1
    return tot_loss / n, tot_bce / n, tot_ber / n


def get_adaptive_model_params(vec_dim: int, msg_len: int):
    base_capacity = vec_dim * msg_len  # ä¿¡æ¯å®¹é‡åŸºå‡†

    # è‡ªé€‚åº”è®¡ç®—æ¨¡å‹æ·±åº¦ (4-16å±‚)
    # é«˜ç»´å‘é‡éœ€è¦æ›´æ·±çš„ç½‘ç»œæ¥å¤„ç†å¤æ‚ç‰¹å¾ï¼Œæ”¯æŒåˆ°8Kç»´åº¦
    # ä½ç»´å‘é‡ç”¨æµ…ç½‘ç»œé¿å…è¿‡æ‹Ÿåˆ
    depth = max(4, min(16, int(4 + 12 * (vec_dim / 1024))))

    # è‡ªé€‚åº”è®¡ç®—éšå±‚å€æ•° (2-10å€)
    # é«˜ç»´å‘é‡å¯ä»¥ç”¨æ›´å¤§çš„éšå±‚æ¥å……åˆ†åˆ©ç”¨ä¿¡æ¯
    # ä½ç»´å‘é‡ç”¨è¾ƒå°éšå±‚æé«˜å‚æ•°æ•ˆç‡
    base_mul = 2 + 8 * (vec_dim / 2048)  # çº¿æ€§å¢é•¿ï¼Œæ”¯æŒæ›´é«˜ç»´åº¦
    hidden_mul = max(2, min(10, int(base_mul)))

    # è‡ªé€‚åº”è®¡ç®—æ‰°åŠ¨å¼ºåº¦ (0.005-0.08)
    # é«˜ç»´å‘é‡å¯ä»¥æ‰¿å—æ›´å¤§æ‰°åŠ¨ï¼Œä½†è¦æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…
    delta_scale = max(0.005, min(0.08, 0.01 + 0.07 * (vec_dim / 2048)))

    # è‡ªé€‚åº”è®¡ç®—dropoutç‡ (0.01-0.4)
    # è¶…é«˜ç»´å‘é‡éœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
    dropout = max(0.01, min(0.4, 0.05 + 0.35 * (vec_dim / 2048)))

    return {
        'depth': depth,
        'hidden_mul': hidden_mul,
        'delta_scale': delta_scale,
        'dropout': dropout,
        'capacity_ratio': base_capacity / (vec_dim * vec_dim)  # ç”¨äºåç»­è°ƒæ•´
    }


def get_adaptive_training_params(vec_dim: int, base_lr: float):
    """
    æ ¹æ®å‘é‡ç»´åº¦è‡ªé€‚åº”è®¡ç®—è®­ç»ƒå‚æ•°

    Args:
        vec_dim: å‘é‡ç»´åº¦
        base_lr: åŸºç¡€å­¦ä¹ ç‡

    Returns:
        dict: åŒ…å«è®­ç»ƒå‚æ•°çš„å­—å…¸
    """
    # å­¦ä¹ ç‡è‡ªé€‚åº” (æ”¯æŒåˆ°8Kç»´åº¦)
    # é«˜ç»´å‘é‡æ¢¯åº¦æ›´å¤æ‚ï¼Œéœ€è¦æ›´å°çš„å­¦ä¹ ç‡
    # è¶…é«˜ç»´å‘é‡éœ€è¦æå°çš„å­¦ä¹ ç‡ä¿è¯ç¨³å®šæ€§
    lr_scale = max(0.2, min(2.0, 1.0 * (512 / vec_dim)))  # æ‰©å±•èŒƒå›´
    enc_lr = base_lr * lr_scale * 1.2  # ç¼–ç å™¨ç¨é«˜
    dec_lr = base_lr * lr_scale * 0.8  # è§£ç å™¨ç¨ä½

    # æƒé‡è¡°å‡è‡ªé€‚åº” (1e-7 åˆ° 5e-4)
    # è¶…é«˜ç»´å‘é‡å‚æ•°é‡å·¨å¤§ï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–
    weight_decay = max(1e-7, min(5e-4, 1e-6 * math.sqrt(vec_dim / 64)))

    # æ¸…æ´è®­ç»ƒæ¯”ä¾‹è‡ªé€‚åº” (15%-60%)
    # è¶…é«˜ç»´å‘é‡ææ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦å¤§é‡æ¸…æ´è®­ç»ƒ
    clean_ratio = max(0.15, min(0.6, 0.2 + 0.4 * (vec_dim / 2048)))

    return {
        'enc_lr': enc_lr,
        'dec_lr': dec_lr,
        'weight_decay': weight_decay,
        'clean_ratio': clean_ratio
    }


def get_adaptive_noise_params(vec_dim: int):
    """
    æ ¹æ®å‘é‡ç»´åº¦è‡ªé€‚åº”è®¡ç®—å™ªå£°å‚æ•°

    Args:
        vec_dim: å‘é‡ç»´åº¦

    Returns:
        dict: åŒ…å«å™ªå£°å‚æ•°çš„å­—å…¸
    """
    # å™ªå£°å¼ºåº¦è‡ªé€‚åº”
    # ä½ç»´å‘é‡å¯¹å™ªå£°æ›´æ•æ„Ÿï¼Œéœ€è¦æ›´æ¸©å’Œçš„è®¾ç½®
    noise_scale = math.sqrt(vec_dim / 256)

    # é«˜æ–¯å™ªå£°å¼ºåº¦
    gauss_base = 0.015 * noise_scale
    gauss_levels = [gauss_base * 0.5, gauss_base, gauss_base * 1.5]

    # é‡åŒ–ç­‰çº§
    quant_base = int(8 + 4 * noise_scale)
    quant_levels = [quant_base + 4, quant_base, max(6, quant_base - 2)]

    # ç»´åº¦é®è”½æ¯”ä¾‹
    mask_base = 0.95 + 0.03 * (1 - noise_scale)
    mask_levels = [min(0.99, mask_base + 0.02), mask_base, max(0.85, mask_base - 0.05)]

    return {
        'gauss_levels': gauss_levels,
        'quant_levels': quant_levels,
        'mask_levels': mask_levels,
        'val_gauss': gauss_base,
        'val_quant': quant_base
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä»æ•°æ®åº“è®­ç»ƒçš„å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_from_database(vectors: np.ndarray, vec_dim: int, db_type: str = "unknown", 
                       epochs: Optional[int] = None, learning_rate: Optional[float] = None, 
                       batch_size: Optional[int] = None, val_ratio: Optional[float] = None,
                       progress_callback: Optional[Callable] = None):
    """ä»æ•°æ®åº“æ•°æ®è®­ç»ƒæ¨¡å‹
    
    Args:
        vectors: ä»æ•°æ®åº“è·å–çš„å‘é‡æ•°æ® (N, D)
        vec_dim: å‘é‡ç»´åº¦
        db_type: æ•°æ®åº“ç±»å‹æ ‡è¯†ï¼Œç”¨äºä¿å­˜è·¯å¾„
        epochs: è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ä½¿ç”¨Config.EPOCHS
        learning_rate: å­¦ä¹ ç‡ï¼Œé»˜è®¤ä½¿ç”¨Config.LEARNING_RATE
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨Config.BATCH_SIZE
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼Œé»˜è®¤ä½¿ç”¨Config.VAL_RATIO
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    # ä½¿ç”¨æä¾›çš„å‚æ•°æˆ–é»˜è®¤å€¼
    if epochs is None:
        epochs = Config.EPOCHS
    if learning_rate is None:
        learning_rate = Config.LEARNING_RATE
    if batch_size is None:
        batch_size = Config.BATCH_SIZE
    if val_ratio is None:
        val_ratio = Config.VAL_RATIO
    
    random.seed(Config.SEED)
    torch.manual_seed(Config.SEED)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch.backends.cudnn.benchmark = True
    
    # æ ¹æ®å‘é‡ç»´åº¦åˆ›å»ºç»“æœç›®å½•
    exp_dir = Config.get_results_dir(vec_dim)
    Path(exp_dir).mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨æ•°æ®åº“æ•°æ®åˆ›å»ºæ•°æ®é›†
    full_set = VectorWatermarkSet(vectors, Config.MSG_LEN)
    val_len = int(len(full_set) * val_ratio)
    train_len = len(full_set) - val_len
    train_set, val_set = random_split(full_set, [train_len, val_len],
                                      generator=torch.Generator().manual_seed(Config.SEED))
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,
                              num_workers=8, pin_memory=True, persistent_workers=True, drop_last=True)
    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,
                            num_workers=4, pin_memory=True)

    # ğŸš€ è‡ªé€‚åº”å‚æ•°è®¡ç®—
    model_params = get_adaptive_model_params(vec_dim, Config.MSG_LEN)
    training_params = get_adaptive_training_params(vec_dim, learning_rate)
    noise_params = get_adaptive_noise_params(vec_dim)
    
    print(f"ğŸ”§ è‡ªé€‚åº”å‚æ•°é…ç½® (ç»´åº¦: {vec_dim}):")
    print(f"  æ¨¡å‹æ·±åº¦: {model_params['depth']}, éšå±‚å€æ•°: {model_params['hidden_mul']}")
    print(f"  æ‰°åŠ¨å¼ºåº¦: {model_params['delta_scale']:.4f}, Dropout: {model_params['dropout']:.3f}")
    print(f"  ç¼–ç å™¨å­¦ä¹ ç‡: {training_params['enc_lr']:.2e}, è§£ç å™¨å­¦ä¹ ç‡: {training_params['dec_lr']:.2e}")
    print(f"  æ¸…æ´è®­ç»ƒæ¯”ä¾‹: {training_params['clean_ratio']:.2%}")

    # æ¨¡å‹ - ä½¿ç”¨è‡ªé€‚åº”å‚æ•°
    enc = AdvVectorEncoder(
        vec_dim, 
        Config.MSG_LEN, 
        depth=model_params['depth'],
        hidden_mul=model_params['hidden_mul'],
        delta_scale=model_params['delta_scale']
    ).to(device)
    
    dec = AdvVectorDecoder(
        vec_dim, 
        Config.MSG_LEN,
        depth=model_params['depth'],
        hidden_mul=model_params['hidden_mul'],
        p_drop=model_params['dropout']
    ).to(device)

    # ä¼˜åŒ–å™¨ - ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡
    enc_opt = torch.optim.Adam(
        enc.parameters(), 
        lr=training_params['enc_lr'], 
        betas=(0.9, 0.999),
        weight_decay=training_params['weight_decay']
    )
    dec_opt = torch.optim.Adam(
        dec.parameters(), 
        lr=training_params['dec_lr'], 
        betas=(0.9, 0.999),
        weight_decay=training_params['weight_decay']
    )

    total_steps = len(train_loader) * epochs
    warm_steps = int(0.05 * total_steps)
    lr_lambda = lambda s: (s / warm_steps if s < warm_steps else
                           0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * (s - warm_steps) / (total_steps - warm_steps))))
    enc_sched = torch.optim.lr_scheduler.LambdaLR(enc_opt, lr_lambda)
    dec_sched = torch.optim.lr_scheduler.LambdaLR(dec_opt, lr_lambda)
    scaler = GradScaler(device, enabled=(device == "cuda"))

    # ---------- è‡ªé€‚åº”å™ªå£°é…ç½® ----------
    val_noise = Compose((
        GaussianNoise(noise_params['val_gauss']), 
        Quantize(noise_params['val_quant'])
    )).to(device)

    noise_pool = []
    # æ·»åŠ ä¸åŒå¼ºåº¦çš„é«˜æ–¯å™ªå£°
    for gauss_level in noise_params['gauss_levels']:
        noise_pool.append(GaussianNoise(gauss_level))
    
    # æ·»åŠ ä¸åŒç²¾åº¦çš„é‡åŒ–å™ªå£°
    for quant_level in noise_params['quant_levels']:
        noise_pool.append(Quantize(quant_level))
    
    # æ·»åŠ ä¸åŒæ¯”ä¾‹çš„ç»´åº¦é®è”½
    for mask_level in noise_params['mask_levels']:
        noise_pool.append(DimMask(mask_level))
    
    for n in noise_pool:
        n.to(device)

    print(f"ğŸ”§ å™ªå£°é…ç½®: {len(noise_pool)}ç§å™ªå£°ç±»å‹")
    print(f"  éªŒè¯å™ªå£°: Gauss({noise_params['val_gauss']:.3f}) + Quant({noise_params['val_quant']})")

    # æŒ‡æ ‡åˆ—è¡¨
    tr_loss, tr_bce, tr_ber = [], [], []
    va_loss, va_bce, va_ber = [], [], []

    best_val_ber = 1e9
    stall = 0

    print(f"å¼€å§‹è®­ç»ƒ {vec_dim}ç»´å‘é‡æ¨¡å‹ï¼Œæ•°æ®æ¥æºï¼š{db_type}")
    print(f"è®­ç»ƒæ•°æ®ï¼š{len(train_set)} æ ·æœ¬ï¼ŒéªŒè¯æ•°æ®ï¼š{len(val_set)} æ ·æœ¬")
    print(f"è®­ç»ƒå‚æ•°ï¼šepochs={epochs}, lr={learning_rate}, batch_size={batch_size}, val_ratio={val_ratio}")

    for ep in range(1, epochs + 1):
        # åŠ¨æ€å™ªå£°ç­–ç•¥é€‰æ‹©
        max_noises_to_compose = min(1 + (ep - 1) // (epochs // 4), 3)

        # ğŸ”§ è‡ªé€‚åº”delta_scaleåŠ¨æ€è°ƒæ•´
        base_delta = model_params['delta_scale']
        growth_factor = 1 + 0.4 * ep / epochs
        enc.delta_scale = min(base_delta * 1.5, base_delta * growth_factor)
            
        lam = lambda_mse(ep, epochs)

        enc.train()
        dec.train()
        ep_l = ep_b = ep_r = 0.0
        n_batch = 0
        
        for cover, msg in train_loader:
            cover = cover.to(device).float()
            msg = msg.to(device).float()
            norm = torch.norm(cover, p=2, dim=-1, keepdim=True)
            cover = cover / (norm + 1e-8)

            # ğŸ”§ è‡ªé€‚åº”å™ªå£°åº”ç”¨ç­–ç•¥
            if random.random() < training_params['clean_ratio']:
                # æ¸…æ´è®­ç»ƒï¼Œæ¯”ä¾‹æ ¹æ®ç»´åº¦è‡ªé€‚åº”
                noise = nn.Identity().to(device)
            else:
                # å™ªå£°è®­ç»ƒï¼Œç»„åˆæ•°é‡æœ‰é™åˆ¶
                num_noises = random.randint(1, min(max_noises_to_compose, len(noise_pool) // 3))
                noises_to_apply = random.sample(noise_pool, k=num_noises)
                noise = Compose(tuple(noises_to_apply))

            with autocast(device, enabled=(device == "cuda")):
                stego = enc(cover, msg)
                stego_noisy = noise(stego)
                logits = dec(stego_noisy)
                probs = torch.sigmoid(logits.float())
                bce = F.binary_cross_entropy_with_logits(
                    logits.float(),
                    msg
                )
                mse = F.mse_loss(stego, cover)
                loss = bce + lam * mse
                
            scaler.scale(loss).backward()
            scaler.unscale_(enc_opt)
            scaler.unscale_(dec_opt)
            torch.nn.utils.clip_grad_norm_(enc.parameters(), 1.0)
            torch.nn.utils.clip_grad_norm_(dec.parameters(), 1.0)
            scaler.step(enc_opt)
            scaler.step(dec_opt)
            scaler.update()
            enc_opt.zero_grad(set_to_none=True)
            dec_opt.zero_grad(set_to_none=True)
            enc_sched.step()
            dec_sched.step()

            ber = (probs > .5).float().ne(msg).float().mean()

            ep_l += loss.item()
            ep_b += bce.item()
            ep_r += ber.item()
            n_batch += 1

        tr_loss.append(ep_l / n_batch)
        tr_bce.append(ep_b / n_batch)
        tr_ber.append(ep_r / n_batch)

        # éªŒè¯
        v_loss, v_bce, v_ber = evaluate((enc, dec), val_loader, val_noise, lam, device)
        va_loss.append(v_loss)
        va_bce.append(v_bce)
        va_ber.append(v_ber)

        print(f"E{ep:03d} | Train loss {tr_loss[-1]:.4f}  BCE {tr_bce[-1]:.4f}  BER {tr_ber[-1]:.3%} "
              f"|| Val loss {v_loss:.4f}  BCE {v_bce:.4f}  BER {v_ber:.3%}")

        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback(ep, epochs, {
                'train_loss': tr_loss[-1],
                'train_bce': tr_bce[-1], 
                'train_ber': tr_ber[-1],
                'val_loss': v_loss,
                'val_bce': v_bce,
                'val_ber': v_ber
            })

        # Early-stop on val BER
        if v_ber < best_val_ber - 0.0005:
            best_val_ber = v_ber
            stall = 0
            model_path = Config.get_model_path(vec_dim)
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            torch.save({'enc': enc.state_dict(), 'dec': dec.state_dict()}, model_path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {model_path}")

    print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯BER: {best_val_ber:.3%}")
    
    # ğŸ”§ è‡ªé€‚åº”æ€§èƒ½è¯„ä¼°æ ‡å‡†
    # æ ¹æ®å‘é‡ç»´åº¦å’Œæ¨¡å‹å®¹é‡è°ƒæ•´è¯„ä¼°æ ‡å‡†
    capacity_ratio = model_params['capacity_ratio']
    
    # åŠ¨æ€è°ƒæ•´æ€§èƒ½é˜ˆå€¼
    excellent_threshold = max(0.005, min(0.02, 0.01 * (1 + capacity_ratio)))
    good_threshold = max(0.02, min(0.08, 0.05 * (1 + capacity_ratio)))
    
    performance_level = "excellent" if best_val_ber < excellent_threshold else \
                       "good" if best_val_ber < good_threshold else "poor"
    
    suggestions = []
    
    if performance_level == "poor":
        # é€šç”¨å»ºè®®
        if best_val_ber > good_threshold * 2:
            suggestions.append(f"éªŒè¯é”™è¯¯ç‡({best_val_ber:.1%})è¿‡é«˜ï¼Œå»ºè®®å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡")
        if len(train_set) < vec_dim * 50:  # åŸºäºç»´åº¦çš„æœ€å°æ ·æœ¬æ•°è¦æ±‚
            suggestions.append(f"è®­ç»ƒæ•°æ®è¾ƒå°‘({len(train_set)}æ ·æœ¬)ï¼Œå»ºè®®å¢åŠ åˆ°è‡³å°‘{vec_dim * 100}æ ·æœ¬")
        
        # åŸºäºæ¨¡å‹å®¹é‡çš„å»ºè®®
        if model_params['hidden_mul'] < 6:
            suggestions.append("æ¨¡å‹å®¹é‡å¯èƒ½ä¸è¶³ï¼Œè€ƒè™‘å¢åŠ hidden_mulå‚æ•°")
        if training_params['clean_ratio'] > 0.3:
            suggestions.append("æ¸…æ´è®­ç»ƒæ¯”ä¾‹è¾ƒé«˜ï¼Œå¯ä»¥å°è¯•å¢åŠ å™ªå£°è®­ç»ƒ")
        
        # é€šç”¨ä¼˜åŒ–å»ºè®®
        suggestions.extend([
            f"é’ˆå¯¹{vec_dim}ç»´å‘é‡çš„ä¼˜åŒ–å»ºè®®:",
            "1. ç¡®ä¿è®­ç»ƒæ•°æ®è´¨é‡é«˜ä¸”åˆ†å¸ƒå‡åŒ€",
            "2. å¯å°è¯•å¢åŠ æ‰¹å¤„ç†å¤§å°ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§", 
            "3. æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®ï¼ˆL2å½’ä¸€åŒ–ç­‰ï¼‰",
            "4. è€ƒè™‘è°ƒæ•´éªŒè¯é›†æ¯”ä¾‹ä¸º0.1-0.2ä¹‹é—´",
            f"5. å½“å‰æ¨¡å‹ä½¿ç”¨{model_params['depth']}å±‚æ·±åº¦ï¼Œå¯å°è¯•Â±1å±‚è°ƒæ•´"
        ])
        
    elif performance_level == "good":
        suggestions.append(f"{vec_dim}ç»´å‘é‡è®­ç»ƒæ•ˆæœè‰¯å¥½ï¼å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯é€‚å½“å¢åŠ è®­ç»ƒè½®æ•°")
    
    # æ€§èƒ½è¯„ä¼°åé¦ˆ
    print(f"\nğŸ“Š æ€§èƒ½è¯„ä¼° (ç»´åº¦: {vec_dim}):")
    print(f"  æ€§èƒ½ç­‰çº§: {performance_level.upper()}")
    print(f"  è¯„ä¼°é˜ˆå€¼: ä¼˜ç§€<{excellent_threshold:.1%}, è‰¯å¥½<{good_threshold:.1%}")
    print(f"  æ¨¡å‹é…ç½®: {model_params['depth']}å±‚ Ã— {model_params['hidden_mul']}å€éšå±‚")
    
    if performance_level == "excellent":
        print(f"ğŸ‰ {vec_dim}ç»´å‘é‡è¾¾åˆ°ä¼˜ç§€æ°´å¹³ï¼")
    elif performance_level == "good":
        print(f"âœ… {vec_dim}ç»´å‘é‡è®­ç»ƒæˆåŠŸï¼")
    else:
        print(f"âš ï¸  {vec_dim}ç»´å‘é‡è®­ç»ƒéœ€è¦ä¼˜åŒ–ï¼Œå»ºè®®å‚è€ƒä»¥ä¸‹å»ºè®®")
    
    return {
        "success": True,
        "best_ber": best_val_ber,
        "model_path": Config.get_model_path(vec_dim),
        "epochs": epochs,
        "performance_level": performance_level,
        "suggestions": suggestions,
        "adaptive_config": {  # è®°å½•ä½¿ç”¨çš„è‡ªé€‚åº”é…ç½®
            "model_params": model_params,
            "training_params": training_params,
            "noise_params": noise_params
        },
        "evaluation_thresholds": {
            "excellent": excellent_threshold,
            "good": good_threshold
        },
        "final_metrics": {
            "train_loss": tr_loss[-1] if tr_loss else 0,
            "train_ber": tr_ber[-1] if tr_ber else 1,
            "val_loss": va_loss[-1] if va_loss else 0,
            "val_ber": best_val_ber
        }
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    freeze_support()
    # Example usage (replace with actual data and parameters)
    # vectors = np.load("path/to/your/vectors.npy") # Replace with your data path
    # vec_dim = 384 # Replace with your vector dimension
    # db_type = "example_db"
    # result = train_from_database(vectors, vec_dim, db_type=db_type, epochs=100, learning_rate=0.001, batch_size=64, val_ratio=0.2)
    # print(result)
